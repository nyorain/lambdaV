\documentclass[letterpaper,12pt]{article}

\usepackage{amsmath,amssymb,graphicx,hyperref,enumerate,bussproofs,turnstile,listings}
\usepackage[dvipsnames]{xcolor}
\usepackage{tabularx}

\usepackage[paper=letterpaper,left=25mm,right=25mm,top=25mm,bottom=25mm]{geometry}
\usepackage{fancyhdr} %% for details on how this work, search-engine ``fancyhdr documentation''
\pagestyle{fancy}

\lhead{CPSC 539B: Compiler Theory} 
\chead{Project: $\lambda$V}
\rhead{Jan}
\cfoot{Page \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\newcommand{\vi}{\vec{I}}

\lstset{ 
	basicstyle=\footnotesize,
	tabsize=2,
}

\begin{document}

\section{Introduction}

We present the translation from a simple high-level language with
functional elements and LISP-like syntax to SPIR-V, an abstract
assembly language for GPU shaders and kernels.

The idea of using functional concepts in a shader came from the fact
that shaders usually have a small and well-defined set of inputs
and outputs and can therefore be written as mostly pure computations.
Originally, another personal motivation for developing a shader language
translatable to SPIR-V was that existing shader languages have some properties
that are undesirable in certain contexts. For instance, to optimize
shaders for performance (which obviously makes sense) many operations
(or: programming errors) like accessing an array or input out-of-bounds, but
even normalizing a zero-length vector or computations involving infinity
are undefined in certain contexts and environments. In practice, this
undefinedness is unobservable in some implementations while it leads to
major errors on other implementations, making development and debugging
extremely hard. The idea was to develop a language that guarantees to
make undefined behavior observable (e.g. by always checking for undefined
behavior conditions at runtime and write output to a buffer should
they be fulfilled). This obviously comes at a potentially huge and
unacceptable runtime cost and brings no advantage for shipped application
but could be quite useful in a development environment, making sure
that shader programs don't trigger undefined behavior.
During the development of this language I realized though that this
is better realized as a completely separate SPIR-V-only pass since it
doesn't depend at all on anything in the source language. Regarding
the correctness proof of the compiler it is furthermore not too interesting,
since it would just introduce additional branches for builtin operations.
The following sections hint at it a couple of times but in the end I only describe
the translation from our source language to SPIR-V, adopting their
(or the runtime's) notions of undefined behavior.

\section{Source language: $\lambda$V}

Our source language is a simple shader language with functional elements.
To keep proofs and formalization minimal, we will only look at a simple
version of $\lambda$V (basically already a subset of what I implemented~\footnote{See 
\url{https://github.com/nyorain/lambdaV} for my experiments with implementing
a real compiler, a lot of the formal proof here is actually quite close
to the compiler source code}): \\
We only look at fragment shaders and only allow a single observation:
one output to the framebuffer. This isn't too much of a simplification
for fragment shaders and modeling writes to multiple framebuffer attachments
as well as inputs shouldn't be too difficult but just more writing work.
Similarly, there isn't a huge difference to other shader types (such as 
vertex or compute shaders) except that those usually produce more or
different observations and therefore one just once again needs to model the
observations in a more complicated way (especially when allowing
arbitrary buffer or image stores as needed to make compute shaders
useful). But I don't expect the simplified model we use here to be too hard
to extend to cover the given cases.

The syntax of $\lambda$V is as simple as possible. Expressions
can be numbers, true or false, an identifier or a list (consisting of zero or
more expressions). There would be no advantage in encoding builtins such
as $+$, $func$ or $if$ into the syntax of the language. Those are simply
pre-defined identifiers. In practice, keeping the syntax this simple has 
the advantage that writing a parser and AST representation is extremely trivial.

\begin{align*}
	e &::= num \:|\: true \:|\: false \:|\: \textit{identifier} \:|\: (l) \\
	l &::= \varnothing \:|\: e\:l
\end{align*}


\section{The target language: SPIR-V}

SPIR-V~\footnote{\url{https://www.khronos.org/registry/spir-v/specs/unified1/SPIRV.html}}
is an SSA-form abstract typed assembly language. It has
similarities to LLVM IR but is more limited: no dynamic dispatch,
i.e. no function pointers, no memory allocations, no stack frame and
most runtimes (i.e. where SPIR-V modules can be used as shaders or
kernels) don't allow recursion, see for instance the Vulkan 
specification\footnote{\url{https://www.khronos.org/registry/vulkan/specs/1.2/html/chap35.html\#spirvenv}}
for this. On the other hand, SPIR-V provides some 
GPU-specific primitives.
It's specification does not give operational semantics - or 
any formal specification - at all but rather describes the layout 
and semantics in plain text. To formally argue about correctness we need 
to model at least some form of operational semantics though. We will
only consider the subset of SPIR-V used by our compiler though.
For instance, our compiler never output any functions (apart from the
main entrypoint) so we don't care for that.

Our judgment looks like this: $M, \vi, ID, ID \rightarrow M, \vi, ID, ID$.
$ID$ is a SPIR-V identifier, i.e. just a number.
$M$ is of the form $[id \mapsto V] ... $ the memory and maps IDs to values. Values can be 
\begin{itemize}
	\item instruction blocks of form $\vi$
	\item values of form $(V, \tau)$, where the second value is the type,
	% \item type expressions, declaring new types. For instance $Float(i)$,
	% 	declaring a floating point type of $i$ bits or $Vec(t, c)$,
	% 	declaring a vector type of scalar type $t$ with $c$ components.
\end{itemize}
$\vi$ is an instruction vector.
Furthermore, the function has inputs and outputs for the current and previous
block IDs, this is needed to resolve SSA phi instructions.

We formally model execution of a (valid) SPIR-V module like this: when the module
is loaded, all constants and types (declared in the header) are loaded
into a memory $M$. All instruction blocks are loaded into the memory as well,
removing the first $OpLabel$ instruction that is only used to identify
the blocks with an id.
The header of the SPIR-V module defines the entry point function.
Execution looks up the first block in the function (functions must start with 
a label defining the block id) $(\vi_{entry}, id_{entry})$ and then behaves as specified
by the operational semantics for $M, \vi_{entry}, id_{entry}, 0)$.

We represent SPIR-V in the standard textual assembly format, with the new ID
defined by the instruction (if any) on the left of the ``='' sign.

\subsection{Control flow}

\begin{prooftree}
	\AxiomC{$M(tid) = \vi_t$}
	\UnaryInfC{$M, (\text{OpBranch}\: id_t), c, p \rightarrow M, \vi_t, id_t, c$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$M(id_c) = (true, Bool)$}
	\AxiomC{$M(id_t) = \vi_t$}
	\BinaryInfC{$M, (\text{OpBranchConditional}\:id_c\:id_t\:id_f), c, p \rightarrow M, \vi_t, id_t, c$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$M(id_c) = (false, Bool)$}
	\AxiomC{$M(id_f) = \vi_f$}
	\BinaryInfC{$M, (\text{OpBranchConditional}\:id_c\:id_t\:id_f), c, p \rightarrow M, \vi_f, id_f, c$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$p = parent_i$}
	\AxiomC{$M(var_i) = V$}
	\AxiomC{$M' = M[id_r \mapsto V]$}
	\TrinaryInfC{$M, (id_r = \text{OpPhi}\:id_{type}\: var_1\:parent_1 \dots var_n\:parent_n, \vi), c, p \rightarrow M', \vi, c, p$}
\end{prooftree}

\subsection{Computations}\label{sec:computations}

Most instructions simply run a computation. Defining the operational
semantics for those is not too interesting and defining all the rules
for the various functions wouldn't be helpful. They all look more or less
like this example we give for floating point addition:

\begin{prooftree}
	\AxiomC{$M(id_1) = (num_1, Num)$}
	\AxiomC{$M(id_2) = (num_2, Num)$}
	\AxiomC{$M' = M[id_r \mapsto fadd(num_1, num_2)]$}
	\TrinaryInfC{$M, (id_r = \text{OpFAdd}\:id_{type}\:id_1\:id_2, \vi), c, p \rightarrow M', \vi, c, p$}
\end{prooftree}

where $fadd$ simply encodes the semantics of the addition of two numbers.
Interestingly enough, SPIR-V does not specify how overflow or special
cases (infinity or NaN arguments) are handled. Instead, this is usually
specified in the runtime environment. We can for instance look once again into the vulkan
specification. It specifies that ``By default, the implementation may 
perform optimizations on half, single, or double-precision floating-point 
instructions that ignore sign of a zero, or assume that arguments 
and results are not NaNs or infinities.'' and ``NaNs may not be generated. 
Instructions that operate on a NaN may not result in a NaN.''.
Basically everything that uses or results in special floating point
values is undefined behavior, more or less. 
Newer SPIR-V versions support a flag signaling that infinities and NaNs 
must be preserved (since SPIR-V 1.4) but we also want to target SPIR-V 1.0
and implementations that do not provide the optional support for
this flag.
Given these non-guarantess, we can't even check for infinity or NaN
\textit{after} we do an operation since then we might already have
triggered undefined behavior, at least that is my interpreation of
this specification. But checking whether an operation might overflow
(or similar) is a pain (maybe not even possible, given that Vulkan and
SPIR-V give their implementations some freedom regarding
rounding of values returned by computations).

I couldn't find a good solution for this. I tried to get
a clarification on this section in the Vulkan spec and found I was not
the first person confused about it, see \href{https://github.com/KhronosGroup/Vulkan-Docs/issues/961}{Vulkan-Docs issue 961}.
Possible solutions included these:

\begin{itemize}
	\item Just make any overflow or similar undefined in the source language
		as well. I don't want to do this since one of the main motivations
		in the first place was to get an target language program
		that is as deterministic as possible (at least detecting triggered
		undefined behavior).
	\item Actually evaluating whether an operation would operate on or
		return infinity or NaN at runtime. For each operation. That's
		such a huge pain. I'm not even thinking about runtime cost here,
		implementing a check that safely evaluates whether addition
		(and multiplication, division, exp, exp2, pow, \ldots) of floating
		point numbers would give such a result seems like a lot of work
		and definitely not a sane solution to me.
	\item Just outputting SPIR-V 1.4 (or using the previously available
		extension) and requiring support for this flag. That is what
		is assumed in the rest of this report and what will probably
		be used in the final implementation of the compiler since my hardware
		supports it and it makes a big issue just go away, basically. 
		Most computations don't ever trigger any undefined behavior
		anymore.
\end{itemize}

There are still some instructions, however, that trigger undefined
behavior. As mentioned in the introduction, we would optimally like
to have \textit{debug} mode of binaries in which triggering
undefined behavior is somehow signaled, helping the programmer
to remove it from programs, even if the implementation they are using
just happens to implement the undefined behavior that does not
result in any problems. But for now, our source language just inherits
the undefined behavior conditions of SPIR-V. In our conclusion we
go into some detail on a better way to make undefined behavior
observable in SPIR-V in general.

\subsection{Ignored instructions}

There are meta-instructions that must be inserted into a SPIR-V module
for correcntess that don't have an impact on the semantics, like
the OpSelectionMerge and OpLoopMerge that provide meta-information
about the control flow. Furthermore there are debug instructions,
allowing to associate source-language line numbers or variables names with
SPIR-V code. We simply ignore all those instructions, treating them
the same way we treat OpNop:

\begin{prooftree}
	\AxiomC{}
	\UnaryInfC{$M, (\text{OpNop}, \vi), c, p \rightarrow M, \vi, c, p$}
\end{prooftree}


\section{Types in $\lambda$V}

$\tau ::= Num \:|\: 
	Bool \:|\: 
	Vec(\{2, 3, 4\}, \{f, b\}) \:|\: 
	Mat(\{2, 3, 4\}, \{f, b\}) \:|\: 
	Rec\:\tau \:|\: 
	PureRec$ \\

Num is a type for all numbers. We don't seperate between integers and floating
point numbers, we just assume all numbers to be floating-point for simplicity
and since GPUs are usually optimized for that anyways. For logical true and
false, we have Bool. Then, we have various Vector and Matrix types (with
dimensions 2, 3 or 4 and floating point ``f'' or boolean ``b'' elements).
We simplicity, we only allow square-sized matrices at the moment, other
ones are rarely needed in shaders anyways. 
The types $Rec \tau$ and $PureRec$ have nothing to do with
recursive types but are just helper types that
allow us to model the restriction we have to put on recursive functions
(namely: only tail-recursion is allowed) while still deducing the types of
recursive expressions. This is probably the only interesting thing about
the type system. How exactly the types are used should become apparent from
the typing rules below.
To allow minimizing the number of rules and distinct cases, we will
write $Vec(1, {b,f})$ as synonym for $Bool$ or $Num$, respectively.

Our typing judgment has the following form: \\
\begin{center}
\fbox{\begin{minipage}{15em}
	\begin{equation*}
	R, A \vdash e : \tau
	\end{equation*}
\end{minipage}}
\end{center}

$R$ is a \textit{recursive context}, as explained below.
$A$ is a tuple of tuple of expressions, representing the current
stack of call arguments.
Both $R$ and $A$ are basically needed as workaround for not typing
functions while allowing them in almost any context (there are some
technical limitations discussed below).
A typing judgement means that $e$ is of type $\tau$ (in context $R$).
We also use $T, U$ as type metavariables.
[I guess this is fairly common but for tuples we write $((a_1 \dots), a_2)$
for $(a_1 \dots a_2)$, i.e. appending to a tuple].

\begin{prooftree}
	t-num
	\AxiomC{}
	\UnaryInfC{$R, \varnothing \vdash num : Num$}
\end{prooftree}

\begin{prooftree}
	t-true
	\AxiomC{}
	\UnaryInfC{$R, \varnothing \vdash true : Bool$}
\end{prooftree}

\begin{prooftree}
	t-false
	\AxiomC{}
	\UnaryInfC{$R, \varnothing \vdash false : Bool$}
\end{prooftree}

\begin{prooftree}
	t-if
	\AxiomC{$\varnothing, \varnothing \vdash e_1 : Bool$}
	\AxiomC{$R, A \vdash e_2 : tau_1$}
	\AxiomC{$R, A \vdash e_3 : tau_2$}
	\TrinaryInfC{$R, A \vdash (if\: e_1\: e_2\: e_3) : \text{rec-match}(tau_1, tau_2)$}
\end{prooftree}

\begin{prooftree}
	t-app
	\AxiomC{$R, (A, (e_1 \dots e_n)) \vdash e_0 : \tau$}
	\UnaryInfC{$R, A \vdash (e_0\:e_1 \dots e_n) : \tau$}
\end{prooftree}

\begin{prooftree}
	t-func
	\AxiomC{$R, A_r \vdash e[e_1 / id_1]\dots[e_n / id_n] : \tau$}
	\UnaryInfC{$R, (A_r, (e_1 \dots e_n)) \vdash (func\:(id_1 \dots id_n)\:e) : \tau$}
\end{prooftree}

\begin{prooftree}
	t-rec-func
	\AxiomC{$R, \varnothing \vdash e_i : \tau_i$}
	\AxiomC{$\vec{\tau_i}, \varnothing \vdash e[e_1 / id_1]\dots[e_n / id_n] : Rec\:\tau$}
	\BinaryInfC{$R, ((e_1 \dots e_n)) \vdash (\text{rec-func}\:(id_1 \dots id_n)\:e) : \tau$}
\end{prooftree}

\begin{prooftree}
	t-rec
	\AxiomC{$n > 0$}
	\AxiomC{$len(\tau_i) = n$}
	\AxiomC{$\varnothing, \varnothing \vdash e_i : \tau_i$}
	\TrinaryInfC{$\vec{\tau_i}, ((e_1 \dots e_n)) \vdash rec : PureRec$}
\end{prooftree}

\begin{prooftree}
	t-let
	\AxiomC{$R, A \vdash e[id_1 / e_1]\dots[id_n / e_n] : \tau$}
	\UnaryInfC{$R, A \vdash (let\:((id_1\:e_1)\dots(id_n\:e_n))\:e) : \tau$}
\end{prooftree}

Substituion is assumed to be context-sensitive (i.e. only substitute those
identifiers that are really meant in that case and not those that are
redefined in a deeper scope), as usually.

The helper function \textit{rec-match} combines two different types in a
recursive context. 

\[
	rec-match(\tau_1, \tau_2) := 
	\begin{cases}
		\tau_1, & \text{for } \tau_1 = \tau_2, \\
		\tau_1, & \text{for } \tau_1 = Rec\:\tau_2, \\
		\tau_2, & \text{for } \tau_2 = Rec\:\tau_1, \\
		\tau_2, & \text{for } \tau_1 = PureRec \land \tau_2 = Rec\:T, \\
		\tau_1, & \text{for } \tau_2 = PureRec \land \tau_1 = Rec\: T, \\
		Rec\:T, &  \text{for } \tau_2 = PureRec \land \tau_1 = T \text{ (where T isn't $PureRec$ or $Rec\, U$)}, \\
		Rec\:T, &  \text{for } \tau_1 = PureRec \land \tau_2 = T \text{ (where T isn't $PureRec$ or $Rec\, U$)}
	\end{cases}
\]

\textit{PureRec} is the type of a \textit{rec} call but
when we have an \textit{if} expression where one branch just results
in a \textit{rec} call, i.e. having type \textit{PureRec} while the
the other branch contains a  value of type $\tau$ (or $Rec\: \tau$), we can deduce 
that the function must return a value of type $\tau$ in general.
But instead of giving this \textit{if} expression then the type $\tau$,
we give it the type $Rec\: \tau$ since the returned value can't be used
for any further computations (except control flow, at the moment this
only means \textit{if}). Furthermore, this type system encodes
the requirement for \textit{rec-func} constructs to have at least one
\textit{rec} call in its body (since typing requires the body of \textit{rec-func}
to be of type \textit{Rec $\tau$}).

When typing expressions inside a \textit{rec-func} construct, the recursive 
context $R$ holds the types the \textit{rec-func} was called with. This means
that one cannot call recursive functions with function objects (since they
are not typed). In practice
this is a technical limitation we cannot overcome since SPIR-V does not support
dynamic dispatch and allowing recursion on arbitrary (possibly different
for each recursive call) functions yields cases where we can't inline
function calls anymore, i.e. can't unroll recursive functions to simple loops.
In practice, we could put a more relaxed restriction on our type system:
It is allowed to call recursive functions with function values as long
as all recursive calls use the same function value. Or even more general:
As long as there is only a finite number of functions used in the
recursive calls (meaning basically that you recurse with newly
instantianted functions in each recursion, it should be somewhat intuitive
that a case like that can't be inlined/unrolled anymore). But that
is a much more complicated restriction, yields a more complex type system
and code generation. And in practice one can simply use workarounds.
For instance:

\begin{lstlisting}
(let ((nat-fold (func (n accum f) (
	let ((body (rec-func (n accum) (
			if (eq n 0)
				accum
				(rec (- n 1) (f accum n))
		))))
		(body n accum)
)))) ... )
\end{lstlisting}

One can use function value parameters in recursive functions
by simply defining a non-recursive wrapper function.

The author did not know about the Curry paradox and functional
recursive combinators (maybe he shouldn't have picked a functional source language)
and one can write recursive expressions like that in $\lambda$V as well.
Below is a (simple) example showing how simple $func$ constructs can
be (ab-)used to get recursion.

\begin{lstlisting}
(let 
	((sumup (func (self n) 
		(if (eq n 0) 0 (+ n (self self (- n 1)))))))
	(sumup sumup 10))
\end{lstlisting}

But those expressions are not well-typed in our source language.
There is no (finite) derivation tree
for the well-typedness of the example expression since we define our
type rules by substituion, meaning that for recursive function constructs
like this one would need an infinitely large derivation tree (independent
from whether or not the expression actually terminates).
In short: we expect programmers to play nice and use the \textit{rec-func}
construct we provide since we can't support arbitrary recursion. Sadly.

There are furtheremore a lot of more uninteresting typing rules for
the builtin primitives such as arithmetic or trigonometric functions.
We annotate those builtins with types (just some examples in the list below):

\begin{itemize}
	\item $Vec(I,f) \rightarrow Vec(I,f)$, e.g. the unary minus, fract, exp
	\item $Vec(I,f) \rightarrow Num$, e.g. length
	\item $Vec(b,f) \rightarrow Bool$, e.g. any-of
	\item $Vec(I,f) \times Vec(I,f) \rightarrow Vec(I,f)$, e.g. plus
	\item $Vec(I,f) \times Vec(I,f) \rightarrow Num$, e.g. distance
	\item $Vec(I,T) \times Vec(I,T) \rightarrow Vec(I,b)$, e.g. less-than or equal
\end{itemize}

Note that the generic $i$ must be the same for all parameters/return types.
This allows us to just give one generic rule for all of those builtins:

\begin{prooftree}
	\AxiomC{$builtin$ of type $(\tau_1 \dots \tau_n) \rightarrow \tau$}
	\AxiomC{$\varnothing, \varnothing \vdash e_i : \tau_i$}
	\BinaryInfC{$R, \varnothing \vdash (builtin\:e_1 \dots e_n) : \tau$}
\end{prooftree}


\section{$\lambda$V operational semantics}

To reason about correctness properties of our translation, we will define
small-step operational semantics for $\lambda$V.

\begin{prooftree}
	\AxiomC{}
	\UnaryInfC{$(let\:((id_1\:e_1)\dots(id_n\:e_n))\:e) \rightarrow e[e_1 / id_1]\dots[e_n / id_n]$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{}
	\UnaryInfC{$(if\:true\:e_2\:e_3) \rightarrow e_2$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{}
	\UnaryInfC{$(if\:false\:e_2\:e_3) \rightarrow e_3$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{}
	\UnaryInfC{$((func\:(id_1 \dots id_n)\:e)\:e_1 \dots e_n) \rightarrow e[e_1 / id_1]\dots[e_n / id_n]$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{}
	\UnaryInfC{$((\text{rec-func}\:(id_1 \dots id_n)\:e)\:e_1 \dots e_n) \rightarrow e[e_1 / id_1]\dots[e_n / id_n][(\text{rec-func}\:(id_1 \dots id_n)\:e) / rec]$}
\end{prooftree}

The reduction rule for builtins are intuitively defined, just copying
the underlying SPIR-V (and target environment, e.g. Vulkan) semantics,
which in turn usually just refer to the IEEE floating point standard.

The design decision we made for handling the (by default ill-defined)
issues like overflow, infinities and NaN's in Section \ref{sec:computations}
is important here since it significantly modifies the semantics of
those builtins in SPIR-V. Otherwise we would transitively introduce a lot of
undefined behavior in our source language.

% TODO: give example!
% TODO: either adapt the list reduction rule below again
%  or somehow evaluate parameters of builtins! won't work otherwise

% TODO: nope, this is a big pile of bullshit. Actually take the time to
% model undefined behavior in source and target language (and
% add the respective checks to the translation). Point out that
% this is something we just do in our formal model and not in the
% actual compiler because just using an extra, translation- and
% source lanauge independent SPIR-V pass makes more sense?
% But to be honest, then why even mention it here at all?
% Maybe just explain this in the motivation and leave it out of this?
% But definitely say something about it here!

%% A goal of our compiler is to make undefined behavior observable but I
%% found trying to prove this quite pointless. We can model undefined
%% behavior in the target language and add some kind of extra observation
%% for this to our operational semantics of the source language. But this
%% makes everything more complicated while not really helping to establish
%% trust in anything, since the main point of every proof would be us
%% identifying each and every source of undefined behavior in the
%% SPIR-V prose-specification.

\newcommand{\conv}{\rightarrow^+}
\newcommand{\red}{\rightarrow^*}

The conversion semantics are fairly simple due to our general list syntax:

\begin{prooftree}
	\AxiomC{$\vdash e \rightarrow e'$}
	\UnaryInfC{$\vdash e \conv e'$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\vdash e_0 \conv e_0'$}
	\UnaryInfC{$\vdash (e_0 \dots) \conv (e_0' \dots)$}
\end{prooftree}

Reduction semantics:

\begin{prooftree}
	\AxiomC{}
	\UnaryInfC{$\vdash e \red e$}
\end{prooftree}

\begin{prooftree}
	\AxiomC{$\vdash e \conv e''$}
	\AxiomC{$\vdash e'' \red e'$}
	\BinaryInfC{$\vdash e \red e'$}
\end{prooftree}

As usually, we define a evaluation function, $eval_\lambda(e) = o$,
that is defined as the observation $o$ that $e$ can be reduced to.
All possible observations in our simplified version of the source language
is a single $Vec(I,f)$, the value written to the framebuffer by the
fragment shader.

\section{Translation}

We model translation as a recursive function \textit{translate}.
We use the usual structure (unordered tuples where each value
instead gets a name) and member-access notation to keep things
somewhat compact.

\begin{center}
\fbox{\begin{minipage}{15em}
	\begin{align*}
		% R_i &::= \{hb: ID, cb: ID\} \\
		BackEdge &::= (ID, ID*) \\
		GE &::= (ID, \tau) \\
		R_i &::= {defs: identifier \rightarrow GE, cont: ID} \\
		R_o &::= BackEdge* \\
		ID &::= \text{SPIR-V numeral ID} \\
		A &::= (ce*)* \\
		G &::= identifier \rightarrow GE \\
		C &::= [ID \mapsto Num]* \\
		\\
		Input &::= \{expr: e, args: A, gen: G, idc: ID, idl: ID, rec: R_i\} \\
		Output &::= \{code: \vi, consts: C, ido: ID, idc: ID, idl: ID, type: \tau, rec: R_o\} \\
		\\
		translate&: Input \rightarrow Output
		% (ce \times D \times R_i \times A \times ID) &\rightarrow (C \times ID \times ID \times ID \times \vi \times R_o) \\
		% (expr \times defs \times r_i \times args \times id_c) &\mapsto (consts \times id_o \times id_t \times id_c \times instructions \times r_o) \\
	\end{align*}
\end{minipage}}
\end{center}

\textbf{Input.expr} is simply the expression to be translated. We extend expressions
by an intermediate type generated during translation, an already translation
expression holding its ID and its type. \\
\textbf{Input.rec} and \textbf{Output.rec} are information needed to generate recursive
functions. Inside a \textit{rec-func} construct, \textbf{Input.rec} is the block 
ID of the continue block (to jump to) and the IDs as well as types (bundled into
our new GE type) for the recursive function arguments. \\
%, as well as the type IDs of its parameters.
\textbf{Output.rec} is a set of blocks and their respective parameter IDs for
recursvie calls (i.e. edges to the continue block). \\
\textbf{Input.args} models the current argument call stack, pretty much the same
way we modeled it for our typing rules. This is once again needed because
we can't translate function expressions on their own. \\
\textbf{ID} represents a SPIR-V ID. \\
\textbf{Input.idc} is the next usable ID. \\
\textbf{Input.idl} is the ID of the current code generation block (i.e. label). \\
\textbf{Output.ido} is the ID holding the result of the translated expression. \\
% \textbf{Output.idt} is the ID of the type of the generated expression. \\
\textbf{Output.idc} is the next usable ID after the translation.
E.g. if a translation of an expression gets $Input.idc = 42$ as input, 
uses IDs 42, 43, 44 and 45, it returns 46 as $Output.idc$. \\
\textbf{Output.idl} is the label the code generation finishes in.
This is different iff code generation inserts a new $OpLabel$ instruction,
i.e. starting a new basic block. \\

% There is one input and one output ($id_c$)
% to the translation function for the next usable id (e.g. if a translation of an
% expression gets 42 as input, uses IDs 43, 44 and 45, it returns 46).
% The $id_o$ output is the ID the value generated by the expression is stored in.
% The $id_t$ output is the type ID of the generated expression.

\textit{Output.consts} is a set of defined constant instructions. In SPIR-V constants
cannot be defined inline but have to be defined in a special section before
the start of the program, that's why keep them as a separate vector.

\textit{Output.type} is the type of the generated expression.

Finally, \textit{Output.code} is the generated SPIR-V instruction vector.

\subsection{Translation: utility}

We define the function as a set of conditions in which $translate(I) = O$
is defined. The next sections will present a set of conditions for each
expression, you can basically imagine each section being one (giant)
derivation rule for the defining judgment $translate(I) = O$
with all the conditions as premises. 

When generating the SPIR-V module from a full $\lambda$V program,
we define all types in the header (giving them IDs) and can therefore define a
function $typeid: \tau \rightarrow ID$ that returns the SPIR-V type ID
associated with a given type in our formalization. 
We define $typeid(Rec \tau) = typeid(\tau)$. The expression $typeid(PureRec)$
is intentionally undefined, it is never needed for well-defined programs.
Similar to the way we declare types once and can then use them via their
IDs during the translation, we also define the constants $true$ and $false$
once and are able to use them during translation. Their translation
rules are not shown below since they are therefore trivial (basically
a no-op, just returning the ID of the respective constant).

We also need a utility function $ioa$, modeling
the \textit{insert or assign} semantic of a mapping (in our compiler
we simply use a hash map):

\[
	ioa(Mapping, id \mapsto v)(id_c) :=
	\begin{cases}
		v & \text{for } id = id_c \\
		Mapping(id_c) & \text{otherwise}
	\end{cases}
\]

We write $ioa(Mapping, (id \mapsto v)^*)$ as a shortcut for subsequent
insertion/replacement, \\
\[
	ioa(ioa( \dots ioa(Mapping, id_1 \mapsto v_1) \dots ), id_n \mapsto v_n)
\]

\subsection{Translation: func}

This is probably the most interesting (and yet one the most simple) translation
rules: to translate a function call we simply translate its body
(effectively always inlining the function) and replace all occurrences of 
function parameter with translations of the expressions bound to them.
Of course, this might lead to code bloat, when huge functions or complex expression passes
as paremters are inlined. This compiler doesn't care too much and
separate optimization passes can still perform common subexpression
elimination or similar (they could technically even refactor code that
is generated multiple times out into its own function, if possible).
The reason we translate functions (or rather: function calls; we can't translate
function in itself, remember how they are not even valid expressions since
not typed at all) like this even though SPIR-V offers functions is that
there is no dynamic dispatch in SPIR-V. Therefore force-inlining basically
everything is the only way to get higher-order functions (well, with
the restrictions outlined in the beginning).

\medskip
\begin{tabularx}{\linewidth}{rl}
	I.expr &= (func ($id_1 \dots id_n$) e) \\
	I.args &= ($A_r$, ($e_1 \dots e_n$)) \\
	O &= translate(\{expr: $e[e_1 / id_1]\dots[e_n / id_n]$, args: $A_r$, idc: I.idc, idl: I.idl, rec: I.rec\}) \\
\end{tabularx}

\subsection{Translation: numbers}

All constants must be declared in the SPIR-V header, we therefore
don't generate code for a constant number but simply return our
constant definition (in O.consts) and return the id of the constant.

\begin{tabularx}{\linewidth}{rl}
	I.expr &= \textit{num} \\
	I.args &= $\varnothing$ \\
	O.code &= $\varnothing$ \\
	O.consts &= [I.idc $\mapsto$ \textit{num}] \\
	O.idc &= I.idc + 1 \\
	O.type &= Num \\
	O.idl &= I.idl \\
	O.rec &= $\varnothing$ \\
\end{tabularx}

\subsection{Translation: let}

\begin{tabularx}{\linewidth}{rl}
	I.expr &= (let (($id_1 e_1) \dots (id_n e_n)$) e) \\
	O &= translate(\{expr: $e[e_1 / id_1]\dots[e_n / id_n]$, args: I.args, idc: I.idc, idl: I.idl, rec: I.rec\}) \\
\end{tabularx}

\subsection{Translation: list}

This translation rule is only useful (and well-defined) when $e_0$ isn't
an atomic expression such as $func$ or a builtin. \\

\medskip
\begin{tabularx}{\linewidth}{rl}
	I.expr &= ($e_0$ $e_1$ \dots $e_n$) \\
	nargs &:= (I.args, ($e_1$\dots$e_n$)) \\
	O &= translate(\{expr: $e_0$, args: nargs, idc: I.idc, idl: I.idl, rec: I.rec\}) \\
\end{tabularx}

\subsection{Translation: if}

This is the first translation actually generating SPIR-V code. The main
complexity in this translation is handling cases where one (or both) of the
given branches is just a recursive call (i.e. of type $PureRec$).

\medskip
\begin{tabularx}{\linewidth}{rl}
	I.expr &= (if $e_c$ $e_t$ $e_f$) \\
	$O_c$ &= translate(\{expr: $e_c$, args: I.args, idc: I.idc + 4, idl: I.idl, rec: I.rec\}) \\
	$O_t$ &= translate(\{expr: $e_t$, args: I.args, idc: $O_c$.idc, idl: I.idc + 0, rec: I.rec\}) \\
	$O_f$ &= translate(\{expr: $e_f$, args: I.args, idc: $O_f$.idc, idl: I.idc + 1, rec: I.rec\}) \\

	O.consts &= consts: $O_c$.consts $O_t$.consts $O_f$.consts \\
	O.ido &= 
	$\begin{cases}
		0 & \text{for } O_f.type = PureRec \land O_t.type = PureRec \\
		I.idc + 4 & \text{otherwise} \\
	\end{cases}$ \\

	O.idl &= 
	$\begin{cases}
		0 & \text{for } O_f.type = PureRec \land O_t.type = PureRec \\
		I.idc + 2 & \text{otherwise} \\
	\end{cases}$ \\

	O.type &= 
	$\begin{cases} 
		O_t.type & \text{for } O_f.type = PureRec \\
		O_f.type & \text{otherwise} \\
	\end{cases}$ \\
	O.idc &= $O_f$.idc \\
	O.rec &= $O_t$.rec, $O_f$.rec \\
\end{tabularx}

\medskip
$O.code$ is defined as: \\
\begin{tabularx}{\linewidth}{rll}
	& $O_c$.code \\
	& OpSelectionMerge (I.idc + 4) None \\
	& OpBranchConditional $O_c$.ido (I.idc + 1) (I.idc + 2) \\
	\cline{1-2}
	(I.idc + 0) = &OpLabel \\
	& $O_t$.code \\
	& OpBranch (I.idc + 2) & \textbf{[if $O_t$.type $\neq$ PureRec]} \\
	\cline{1-2}
	(I.idc + 1) = &OpLabel \\
	& $O_f$.code \\
	& OpBranch (I.idc + 2) & \textbf{[if $O_f$.type $\neq$ PureRec]} \\
	\cline{1-2}
	(I.idc + 2) = &OpLabel & \textbf{[if $O$.type $\neq$ PureRec]} \\
	(I.idc + 3) = &OpPhi typeid(O.type) & \textbf{[if $O$.type $\neq$ PureRec]} \\
		& \quad $O_t$.ido ($O_t$.idl) & \quad \textbf{[if $O_t$.type $\neq$ PureRec]} \\
		& \quad $O_f$.ido ($O_f$.idl) & \quad \textbf{[if $O_f$.type $\neq$ PureRec]} \\
\end{tabularx}

\subsection{Translation: Computations}

The various builtins are intuitively translated to a single instruction.
For instance, a translation of the ``vec4'' builtin, using
4 numbers to construct a \textit{Vec(4,f)} could look like this:

\medskip
\begin{tabularx}{\linewidth}{rl}
	I.expr &= vec4 \\
	I.args &= (($e_1$ $e_2$ $e_3$ $e_4$)) \\
	$O_0$ &= \{idc: I.idc + 1\} \\
	$O_i$ &= translate(\{expr: $e_i$, args: $\varnothing$, idc: $O_{i - 1}$.idc, idl: I.idl, rec: I.rec\}) \dots \\
	O.idc &= $O_4$.idc \\
	O.ido &= I.idc + 1 \\
	O.type &= Vec(4, f) \\
	O.rec &= $\varnothing$ \\
	O.consts &= $O_i.consts$ \dots \\
	O.code &= (I.idc + 1) = OpCompositeConstruct typeid(Vec(4, f)) $O_1$.ido $O_2$.ido $O_3$.ido $O_4$.ido \\
\end{tabularx}

\newpage
\subsection{Translation: rec-func}

Translating \textit{rec-func} is by far the most complicated translation.
Since most SPIR-V runtimes don't allow recursion (GPUs traditionally
don't have a stack) we have to unroll it into a loop. That is the reason
we only support this limited form of recursion in our source language.
We provide the frame, including all basic blocks and SSA phi functions
for recursive calls from within the function body, i.e. (after translation)
jumps back to the begin of the loop body (via a separate continue block SPIR-V needs)
from within the loop body.

\medskip
\begin{tabularx}{\linewidth}{rl}
	I.expr &= (rec-func\: ($id_1 \dots id_n$) e) \\
	I.args &= (($e_1 \dots e_n$)) \\
	\\
	hb &:= I.idc + 0 \\
	lb &:= I.idc + 1 \\
	cb &:= I.idc + 2 \\
	mb &:= I.idc + 3 \\
	$O_0$ &:= \{idc: I.idc + 4\} \\
	$O_i\dots$ &:= translate(\{expr: $e_i$, args: $\varnothing$, idc: $O_{i - 1}$.idc, idl: I.idl, rec: I.rec\}) \dots \\
	$D_b$ &:= ioa(I.rec.defs, ($id_i$ $\mapsto$ ($O_n.idc$ + i, $O_i$.type))\dots) \\
	$O_b$ &:= translate(\{expr: e, idc: $O_n.idc + 2n$, idl: lb, rec: {cont: cb, defs: $D_b$}\}) \\
	\\
	$O_b$.type &= Rec $\tau$ \\
	$O_b$.rec &= $((block_1, bid_1^1 \dots bid_n^1) \dots (block_m, bid_1^m \dots bid_n^m))$ \\
	\\
	O.rec &= $\varnothing$ \\
	O.consts &= $O_b$.consts $O_i$.consts\dots \\
	O.ido &= $O_b$.ido \\
	O.idl &= mb \\
	O.type &= $\tau$
\end{tabularx}

\medskip

Furthermore, $O.code$ is defined as: \\
\begin{tabularx}{\linewidth}{rll}
	&OpBranch hb \\
	&$O_i$.code & \textbf{[for i = 1..n]} \\
	\cline{1-2}
	hb = &OpLabel \\
	($O_n$.idc + i) = &OpPhi\: typeid($O_i$.type) $O_i$.ido $I$.idl ($O_n$.idc + n + i) cb & \textbf{[for i = 1..n]} \\
	&OpLoopMerge mb cb None \\
	&OpBranch lb \\
	\cline{1-2}
	lb = &OpLabel \\
	&$O_b$.code \\
	&OpBranch mb \\
	\cline{1-2}
	cb = &OpLabel \\
	($O_n$.idc + n + i) = &OpPhi typeid($O_n$) & \textbf{[for i = 1..n]} \\
		&\quad $block_k$ $bid_i^k$ & $\quad$\textbf{[for k = 1..m]} \\
	&OpBranch hb \\
	\cline{1-2}
	mb = &OpLabel
\end{tabularx}


\subsection{Translation: rec}

The definition of this \textit{rec} translation only makes sense together
with the translation of \textit{rec-func}. Interesting here is that
this translation does not return a value or block ID (dummy value 0) in 
O.ido, O.idl, since neither is defined.
For well-typed expressions this will never again be needed during
the translation since the result of this expression can't be used
anyways, that is the idea of tail recursion.
With our current type system, every \textit{rec} expression must be wrapped
immediately into an \textit{if} expression.

\medskip
\begin{tabularx}{\linewidth}{rl}
	I.expr &= rec \\
	I.args &= (($e_1$\dots$e_n$)) \\
	$O_0$ &= \{idc: I.idc\} \\
	$O_i$ &= translate(\{expr: $e_i$, args: $\varnothing$, idc: $O_{i - 1}$.idc, idl: I.idl, rec: $\varnothing$\}) \dots \\
	\\
	O.idc &= $O_n$.idc \\
	O.ido &= 0 \\ 
	O.idl &= 0 \\
	O.type &= PureRec \\
	O.consts &= $O_i.consts$ \dots \\
	O.rec &= (I.idl, ($O_i$.ido \dots)) \\
	O.code &= $O_i.code$\dots \: OpBranch I.rec.cont \\
\end{tabularx}

\subsection{Translation: identifier}

We replace all identifiers except the function parameters in \textit{rec-func}
constructs, since those cannot be replaced by source-lanauge expressions
but must simply use the results from the \textit{OpPhi} instructions
in the beginning of the \textit{rec-func} loop body.

\medskip
\begin{tabularx}{\linewidth}{rl}
	I.expr &= \textit{identifier} \\
	I.args &= $\varnothing$ \\
	I.rec(\textit{identifier}) &= (\textit{ido}, \textit{tau}) \\
	O &= \{code: $\varnothing$, consts: $\varnothing$, ido: ido, idc: I.idc, idl: I.idl, type: $\tau$, rec: $\varnothing$\} \\
\end{tabularx}

\subsection{Module creation}

A valid SPIR-V module requires more than just the instruction vector.
We have to add a header, define an entry point, the used extensions,
all types and constants (generated/used by the translation) as well as the one
output variable (representing the framebuffer output, i.e. where we will
write the resulting value in the end) which must be annotated with
an $OpDecorate$ instruction that connects it to the framebuffer output.
Then we define the entry point function and the first block using
$OpLabel$. After that we insert the code generated by the translation
of the source expression. In the end we have to store the SPIR-V value
in the global output using $OpStore$ on O.ido from our translation.
Then we end our main function using $OpReturn$ and $OpFunctionEnd$.

We statically use IDs for types and the entry block. We therefore
define the first usable ID (i.e. the number of IDs we statically use in
the header + 1) as \textit{startid} and the ID of the first basic
block (\textit{OpLabel}) in our main function as \textit{startblock}.
Both of these are required in the translation of our source
expression.

\section{Correctness}

\textit{Note: the proof regarding the tail recursion (i.e. involving \textit{rec})
is not completely formalized and has some problems. I had trouble finding a formalization
in which this can obviously and easily be proven.}
\medskip

We want to prove whole program correctness. Valid programs are expressions
that are well-typed with no recursive context and no arguments and
have a type $\tau_o$ that qualifies as observation, i.e. $Vec(i, f)$, since only
those values can be returned (written into a framebuffer) by a fragment shader.
We furthermore define the utility function
$init_S: (C \times \vi) \rightarrow M $ that returns
a SPIR-V memory object initialized with the constants from $C$ and the 
block mappings from the full-program vector given in the $\vi$ argument.
Another utility function $eval_S: (M \times ID) \rightarrow Vec(1, f)$
models the full evaluation of the program loaded into the present SPIR-V
memory and returns the observation mapped to the given ID after
program execution according to the SPIR-V operational semantics we
outlined. It basically starts execution at the block \textit{startblock}
and applies the rules from the SPIR-V operational semantics until
no instructions are left and then returns the value present in the
memory for the given ID.

Another utility function to connect definitions and substitution:
$subst: (e \times D) \rightarrow e$ is defined as
$e[e_i \ id_i]\dots$ for every $id_i \mapsto e_i$ mapping in the
given definitions mapping.

The correctness theorem looks like this:

\begin{center}
\fbox{\begin{minipage}{0.9\textwidth}
	\begin{flushleft}
	When we have a well-typed program $e$ with  \\
	$\varnothing, \varnothing \vdash e : \tau_o$ and  \\
	$eval_\lambda(e) = o$ in $\lambda$V and  \\
	$translate(\{expr: e, args: \varnothing, idc: \textit{startid}, idl: \textit{startblock}, rec: \varnothing\}) = O$, \\
	then $eval_S(init_S(O.consts, O.code), O.ido) = o$.
	\end{flushleft}
\end{minipage}}
\end{center}
\medskip
To actually prove this we need to strengthen the induction argument,
accounting for arguments and types:

\begin{center}
\fbox{\begin{minipage}{1\textwidth}
	When we have a well-typed program $e$ with \\
	$\varnothing, ((e_1^1 \dots) \dots (e_n^1 \dots)) \vdash e : \tau$ and  \\
	$eval_\lambda(((e\: e_n^1 \dots) \dots)e_1^1 \dots) = o$ in $\lambda$V and  \\
	$translate(\{expr: e, args: ((e_1^1 \dots) \dots (e_n^1 \dots)), idc: \textit{startid}, idl: \textit{startblock}, rec: \varnothing\}) = O$, \\
	\textbf{then} \\
	$O.type = \tau$, \\
	if $O.ido \neq 0$, it has type $idtype(\tau)$; \\
	and $eval_S(init_S(O.consts, O.code), O.ido) = o$.
\end{minipage}}
\end{center}
\medskip

We did not formally define the type system for SPIR-V but it should
be obvious from its text specification. \\
We prove this theorem by induction over the well-typedness, i.e. the type
derivation tree of expression $e$. We can start with the simple
cases, even without additional Lemmas.

\subsection{Correctness: numbers}

When $e$ is a number \textit{num} it obviously evaluates to itself
as observation. Looking up its translation rule, we see that it translates
to no code at all. But since $eval_S(init_S(O.consts, O.code), O.ido)$
returns the value of $O.ido$ --- in case of the translation of a constant that's simply
the ID we gave the constant --- in the memory after execution --- which
in our case is the same memory as before the execution --- we simply
get \textit{num} since that's the constant we added in our translation,
that gets loaded into initial memory by $init_S$.
We could trivially formalize this proof given formalizations of 
$eval_S$ and $init_S$ but the proof would just trivially look like
described here.

\subsection{Correctness: computations}

Since we defined all builtins to just have the semantics of their
SPIR-V counterparts (see the example for the operational semantics of OpFAdd,
all we do is basically saying "this function behaves like specified in
SPIR-V"), the correctness proof for those is trivial. We use the
induction hypothesis for the arguments passed to the builtins.

To formally prove this we would have to strengthen our induction
argument, stating that $O.type$ is the same as the type of $e$ and that
the $O.ido$ is a value of this type as well. Otherwise we can't guarantee 
that the types actually match and that the instruction is valid.
But this can be verified separately or via a Lemma by just looking at
each translation rule.

% \subsection{Correctness: reduction lemma}
% 
% When $e$ reduces to $e'$

\subsection{Correctness: list}

% TODO: we don't actually need the operational semantics here?!
% When $e$ is a list of form $(e_0\: e_1 \dots e_n)$ and $e_0$ can be reduced
% to $e_0'$, the operational semantics specify that $e$ can be reduced to
% $(e_0' e_1 \dots e_n)$. Furthermore, given our typing rule "t-app",
% we can inductively assume the correctness theorem for $e_0$ with arguments
% $(e_1 \dots e_n)$. Here, we once again have to strengthen our induction
% argument to argue about arguments: When an expression $e$ is only
% well-typed under arguments $((e_1^1 \dots e_1^{n_1}) \dots (e_m^1 \dots e_m^{n_m}))$,
% then the evaluations of a translation with any arguments 
% $((d_1, (g_1^1 \dots g_1^{n_1})) \dots (d_m, (g_m^1 \dots g_m^{n_m})))$
% so that (for all i = $1..m$; for all k in $1..n_i$) $g_i^k$ with all identifier
% mappings from $d_i$ substituted is the same as $e_i^k$ must evaluate
% to the same as $((\dots(e\: e_m^1 \dots e_m^{n_m}) \dots) e_1^1 \dots e_1^{n_1})$.
% 
% Given the translation rule for list and our induction hypothesis
% given by the type derivation, we know that the correctness theorem
% holds for $e_0$ with arguments $e_1 \dots e_n$, meaning its translation
% evaluates to the same as $e_0$ with those arguments.
% TODO: not a clear proof
% TODO: what about value replacements except the first?
% need to re-add that to rules

Premises:
\begin{itemize}
	\item $e$ = $(e_0\: e_1 \dots e_n)$, well typed under arguments $A_r$
	\item Per induction hypothesis (see typing rule \textit{t-app}), we
		know that our strengthened theorem holds for $e_0$ with arguments
		$(A_r, (e_1 \dots e_n))$.
\end{itemize}

This translation is basically just a utility step, moving arguments
from the list into our translation argument stack (i.e. we translate
$e_0$ with new argument stack $(A_r, (e_1 \dots e_n))$). \\
The proof for the correctness of the translation is immediately given transitively
by our induction hypothesis, since $(e_0\: e_1 \dots e_n)$ with arguments 
$A_r$ has the same type as $e_0$ with arguments $(A_r, (e_1 \dots e_n))$
and furthermore $e$ with applied argument stack $A_r$ (as seen in the
premise of our strengthened theorem) is syntactically
exactly the same as $e_0$ with unwrapped argument stack $(A_r, (e_1 \dots e_n))$,
meaning they both obviously evaluate to the same observation.

\subsection{Correctness: func}

Premises:
\begin{itemize}
	\item $e$ = $(func\: (id_1 \dots id_n)\: e_b)$, well typed
		under arguments $(A_r, (e_1 \dots e_n))$
	\item Per induction hypothesis (see typing rule \textit{t-app}), we
		know that our strengthened theorem holds for $e_b[e_1 / id_1]\dots[e_n / id_n]$ with arguments
		$(A_r)$.
\end{itemize}

Our translation is defined as 
\textit{O = translate(\{expr: $e[e_1 / id_1]\dots[e_n / id_n]$, args: $A_r$,
idc: I.idc, idl: I.idl, rec: I.rec\})}. Since per typing rules,
$e$ and $e[e_1 / id_1]\dots[e_n / id_n]$ have the same type and
per operational semantics, $e$ can be reduced to
$e[e_1 / id_1]\dots[e_n / id_n]$ i.e. both evaluate to the same value,
we can just directly use our induction hypothesis to prove that
this translation step is correct.

\subsection{Correctness: let}

Premises:
\begin{itemize}
	\item $e$ = $(let\: (\dots(id_i\: e_i)\dots)\: e_b)$, well typed
		under arguments $A$
	\item Per induction hypothesis our strengthened theoerem
		holds for $e_b[e_i / id_i]\dots$, with the same arguments $A$
\end{itemize}

Our translation once again allows us to directly use the induction
hypothesis to prove correctness, since we simply translate it to
the translation of $e_b[e_i / id_i]\dots$ with the same argument.
The operational semantics give us that $e$ can be reduced to
$e_b[e_i / id_i]\dots$ i.e. both evaluate to the same value.
The typeing rules guarantee us that both have the same type.

% Now $e$ is of form $(let\: (\dots(id_i\: e_i)\dots)\: e_b)$. 
% Per induction hypothesis and type rule we can assume our correctness argument
% for $e$ with substituted identifiers. To show that this substitution
% is the same as adding the definitions to our $I.defs$ mapping that
% we translate $e_b$ with (i.e. what our translation does), we define a Lemma:
% 
% So per induction hypothesis we know that $eval_\lambda(e[e_i / id_i]\dots)$
% is the same as the evaluation of the translation in SPIR-V (technically
% we have to apply the current arguments from our strengthened induction
% argument to both). But our Lemma gives us that the translation of
% $e[e_i / id_i]\dots$ evaluates to the same value as $e$ translated with
% $id_i \mapsto e_i\dots$ definitions, which is exactly how
% we defined the \textit{let} translation. Furthermore, our operational
% semantics give us that $(let\: (\dots(id_i\: e_i)\dots)\: e_b)$ can
% be reduced to $e_b[e_i / id_i]\dots$, i.e. they both evaluate to
% the same value.

\subsection{Correctness: if}

Our premises:
\begin{itemize}
	\item $e$ = (if $e_c$ $e_t$ $e_f$), well typed under arguments
		$((e_1^1\dots)\dots(e_n^1\dots))$.
	\item Per induction hypothesis we can assume the correctness
		theorem for $e_c$, $e_t$ and $e_f$
	\item There are two possible cases for the reduction of $e$:
		if $e_c$ evaluates to true, $e$ evaluates to whatever
		$e_t$ evaluates to otherwise whatever $e_f$ evaluates to
		(given the arguments)
\end{itemize}

We step through the code generated by the translation of $e$ using
our SPIR-V operational semantics: \\
First, $O_c$.code is executed, per induction hypothesis
this loads the observation of $eval_\lambda(e_c)$ into the memory
at id $O_c$.ido. Based on our typing rules and the (strengthened) induction hypothesis
applied to $e_c$ we know that $O_c$.ido must be of type bool and the
following \textit{OpBranchCondition} can therefore be reduced.
\textit{OpSelectionMerge} has no effect, and can effectively be ignored
in our reduction, as previously described.
If $O_c$.ido was true, we will execute $O_t$.code, otherwise
$O_f$.code. Per induction hypothesis, both are guaranteed to load
the observation $eval_\lambda(e_t)$ (or $eval_\lambda(e_f)$, respectively)
into $O_t$.ido (or $O_f$.ido, respectively), assuming that the types of the
branch expressions are not \textit{PureRec} (we get to that case below).
After that, control always branches to the block \textit{I.idc + 2}.
All that is done there is to select the computed value via an \textit{OpPhi}
instruction. This value is the return value of the translation. \\

If one of the branch expressions is of type PureRec, that means per
typing rules that it is (potentially nested in some called \textit{func}
or \textit{let} expressions) an expression \textit{(rec ...)}
and can therefore not be reduced (we have no reduction rule for \textit{rec}).
So if this branch is executed, we don't have to fulfill anything
for our output (the premises of the correctness theorem are not fulfilled).
On the other hand, this means (per induction hypothesis)
that its translation will return O.type \textit{PureRec} and can return
no output id, so we must not use its \textit{O.ido} or \textit{typeid(O.type)}.
It furthermore will not follow linear flow, i.e. end the current code block,
so the next instruction afterwards must be \textit{OpLabel}, starting
a new block, for our SPIR-V code vector to be well defined. \\

The returned value $O.ido$ is either $0$ (in the case
when both branches are of type \textit{PureRec}) or of type \textit{typeid(O.type)}.
When either of the branches is not of type \textit{PureRec}, a new
ID will be returned as output, defined by the \textit{OpPhi} instruction
in the last block. This instruction is of type \textit{typeid(O.type)}.
Furthermore, it is well-defined since all passed parameters are
of the same type (remember that $typeid(Rec \tau) = typeid(\tau)$),
that is guaranteed by our typing rules and the induction hypothesis
applied to the (non-\textit{PureRec}) branch expressions.

\subsection{Correctness: rec-func and rec}

This is by far the most complicated proof. We have to prove \textit{rec-func}
and \textit{rec} together because they depend on each other and because
\textit{rec} in itself cannot be reduced, it will always be substituted
instead by the \textit{rec-func} expression it is contained in.

Our premises:
\begin{itemize}
	\item $e$ = (rec-func ($id_1$ \dots) $e_b$)
	\item The arguments for the well-typedness of $e$ are $((e_1 \dots e_n))$.
	\item $e$ reduces to 
		$e_b[e_1 / id_1]\dots[e_n / id_n][(\text{rec-func}\:(id_1 \dots id_n)\:e) / rec]$
	\item Per induction hypothesis we know that our correctness theorem
		holds for \\
		$e_b[e_1 / id_1]\dots[e_n / id_n]$, translated with no arguments.
	\item Per induction hypothesis we also know that our correctness theorem
		holds for all the arguments $e_i$.
\end{itemize}

We reduce the code generated by the translation of $e$ according
to the SPIR-V semantics we defined (writing a derivation
tree for this would require a lot of space): \\
Since we know the correctness theorem holds for the function arguments, 
we know that $O_i.code$ in the $I.idl$ block indeed produces the same observations produced by
$eval_\lambda(e_i)$. There is a reduction rule giving us in
addition basically that $(\dots e_i \dots)$ is the same as
$(\dots o \dots)$, given that $eval_\lambda(e_i) = o$.
When we get to \textit{OpPhi} in block \textit{hb},
$O_i$.ido will be selected since we jumped to the block from \textit{I.idl}.
We ignore the OpLoopMerge instruction and jump to \textit{lb},
where $O_b$ gets executed. The only way $O_b$ won't reach
the \textit{OpBranch} ending the block is if execution gets to the
translation of \textit{rec} (no other translations will ever branch out of
linear flow) associated with the current \textit{rec-func} frame.
Per opertional semantics, \textit{rec} in that case will get substituted 
by the original \textit{rec-func} expression. \\
Our translation of \textit{rec} will
generate the code for all its arguments (here we can use the
induction hypothesis for \textit{rec} to argue that the generated translations
produce the same observations) and branch to \textit{cb}. In \textit{cb},
those arguments will be selected via \textit{OpPhi}, and control
will branch to \textit{hb} again, where those same arguments
(transitively through the previous \textit{OpPhi}) function will
be selected for the arguments used by $O_b.code$, where control
branches subsequently. \\
This means, given that our correctness theorem holds for
$e_b[e_1 / id_1]\dots[e_n / id_n]$ (which we know per induction
hypothesis for \textit{rec-func}), we know that once control reaches
the \textit{OpBranch mb} instruction, the observation stored in
$O_b$.ido (which is what is returned in $O$.ido) should indeed
hold the observation $e$ can be reduced to. \\
The well-typedness of our translation (as required by our strenghtened
induction argument) is similarly guaranteed by the our induction
hypothesis, i.e.\ the well-typedness of the function body and its
arguments.

This argumentation isn't yet enough for a formal proof but with
enough time (and maybe some needed re-formalization) it should
be possible to transform this reasoning into a formal proof,
given that most of our explanation is just applying the SPIR-V operational
semantics we defined.

\section{Conclusion}

We presented a translation for a language with functional elements
such as (tail) recursion and higher-order functions (with restrictions)
to SPIR-V, a language without dynamic memory allocation, recursion,
a stack frame or dynamic dispatch.

Initially, we modeled translation without using our ``magical'' 
context-sensitive substitution. Instead, we used an additional
translation input parameter holding all definitions and passing
the environment for each argument tuple. While that was the last step to basically
make translation just a one-to-one formal model of our code, it made
the formalization much more complicated, requiring multiple helper functions,
an additional strenghtening of our correctness induction hypothesis
and in the end everything just boiled down again to a Lemma saying that
translation of an expression with mappings given in the ``definitions''
argument is the same as just substituting them. \\
We still believe though that it would be worth to verify this
Lemma (i.e. verify that substitution is indeed done correctly by our code)
since this task was not trivial while writing the compiler.

Further work includes extending this formal model from the subset
to a full-blown shader language with inputs, multiple outputs as
well as buffer and image read/write (modeled as inputs or observations,
respectively). We would also like to explore how far other functional
features such as sum types and pattern matching can be translated
to SPIR-V. To actually make this compiler presented here usable,
we would have to explore in how far the existing optimization tools
for SPIR-V binaries are able to minimize the code bloat and performance
problems introduced by our translation.
The \textit{spirv-opt} program delivered promising results in first experiments.
It would be interesting to see if there are optimizations it cannot perform
that could instead be done in our compiler. On the other hand, using additional
(unverified) optimization passes obviously might mess with the outlined correctness
of our compiler.

Furthermore, our SPIR-V binaries (like the output of all shader languages
known to me) can contain undefined behavior. I want to look
into a general SPIR-V compiler pass that just transforms binaries
into a \textit{debug} mode, inserting runtime checks for
undefined behavior conditions, writing
detected problems to a storage buffer object (e.g. writing
an ID associated with the condition and the source location).
There already exists a framework for SPIR-V compiler/optimization passes (that even
implements outputting debug values to a storage buffer) in the SPIRV-Tools 
project repository~\footnote{https://github.com/KhronosGroup/SPIRV-Tools} (of
which \textit{spirv-opt} is a part which could probably be easily extended 
by this kind of pass.

\end{document}
